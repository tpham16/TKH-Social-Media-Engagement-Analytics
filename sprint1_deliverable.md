# Sprint 1 Deliverable

## GitHub Repository

Create a GitHub repository, invite your team-members, and post a link to this repository below

[place link here]

## DataSet Research

For this sprint, you will discover datasets to analyze for your problem through the following links:
* [Google Datasets](https://datasetsearch.research.google.com/)
* [NYC Open Data](https://opendata.cityofnewyork.us/)
* [California Open Data](https://data.ca.gov/)
* [Kaggle](https://www.kaggle.com/datasets)  

As well as:
* making requests for TKH data
* plans for web-scraping/ API access

Keep in mind, even if you plan on requesting data, you can always supplement your analysis with more data found through open resources. 

## DataSet Documentation

For each dataset that you find and are able to download locally, document the following: 

* Quality
* Columns & their definitions
* Dimensions
* Limitations
* Time-Period
* Source

Also, explain where you will save this dataset. Keep in mind, the Amazon database is still eligble for use.

## Minimal Viable Model

We will also be generating a plan for a minimal viable model (MVP). This will be an ETL pipeline that does the BARE MINIMUM:

* Extracts data from some resource
* Does some quick data transformations
* Loads data back in to database
* Predict on data using simple machine learning model

This will be the bedrock of your project that you will use to develop all other features.

Plan out the specificities of this MVP.
* Where is the data coming from? (A csv file is perfectly fine)
* What are some quick data transformations you can implement.
* Which machine learning model will you use?

Keep in mind, this is a quick implementation of your idea! It does not have to be perfect, but it does have to show somewhat interesting predictions from one end to another.
